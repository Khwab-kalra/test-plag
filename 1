Deep Learning based Contactless Biometric
Recognition of Bracelet Lines
Nitin Gupta, Ritwik Duggal, Aarya Pandya∗
∗Department of Computer Science and Engineering, National Institute of Technology, Hamirpur, Himachal Pradesh, India
Emails: nitin3041@gmail.com, ritwikduggal@ieee.org, aaasp16@ieee.org
Abstract—Biometrics has always been an essential component
for the identification of personnel in every sector, even devices
such as smartphones, and smart devices use it these days.
Numerous methods have been implemented for it over the
years, such as biometrics using fingerprint recognition, facial
recognition and in many recent works even using the finger
knuckles. Contactless Biometrics has been more prevalent over
the recent years due to the Covid Pandemic but has paved the way
for more efficient and effective biometric systems. In the proposed
work, a new method is proposed of using the Bracelet lines on
the wrist to identify a person by applying the YOLO model for
detection of wrists along with Machine Learning Techniques for
recognition and comparing other related works and techniques,
the system is created using an Arduino Uno paired with Esp32
CAM which clicks and send the photo of the user to the server for
identification. The proposed work is efficient enough to provide
real-time recognition of the user.
Index Terms—Internet of Things, Machine Learning, YOLO,
Contactless Biometrics, Recognition
I. INTRODUCTION
Biometrics can be defined as the measurement and the
statistical of the behavioural characteristics which make a
person unique, the field of biometrics goes beyond just the
field of authentication but for the proposed work the focus is
mainly on the recognition using biometrics [1].
Over the years, with the increasing technological advancements, authentication has been increasingly demanded in almost every sector, there have been numerous works such as
recognition using knuckles [2], fingerprints [3], facial recognition [4], etc. However, all of these solutions have drawbacks,
such as tattoos, changes due to growth and due to the small
size of the fingerprints and knuckles it is very difficult to
do proper analysis and expensive technology is required the
capture these.
Bracelet Lines are the lines below the palm of a hand, these
are unique and distinct due to the patterns of the lines, which
do not change even through the long course of aging and as
the area is distinctive, it is easy to detect in images and the
proposed solution can be easy to use.
For our solution, we created a dataset of 1400+ images of
people of different age groups as discussed in the sections
below, which have been used to detect the bracelet lines using
the YOLO algorithm with an accuracy of 99.8%. After the
detection various Machine Learning models were deployed
such as SVM with an accuracy of 82.4%, Random Forest
with an accuracy of 84.2% and out of all the models, the best
was given by Neural Network with an accuracy of 98.8%.
Thus, making the Neural Network model extremely reliable
for deployment as discussed in the sections below.
For the deployment, an Arduino Uno R3 is used with an
ESP32-CAM for clicking and sending the images to the server
where the detection and recognition models are deployed.
The proposed work aims to:
1) Create a low-cost solution for authentication
2) Creation of a dataset of images that can be uploaded to
the research community
3) YOLO based solution based proper detection of bracelet
lines
4) Machine Learning based solution for the recognition of
a person using bracelet lines
Solutions based using the wrist/arm areas have been implemented before, but using different techniques and purposes,
in some solutions veins [5] have been detected and in some
cases, it is used to identify terrorists [6].
All of the work has been discussed in detail, in the different
sections of this proposed work. The flow of the paper is
organized in the following order. Section II provides an
overview of the techniques used for biometric identification
and related work done in this area. Section III introduces
the method for data collection, bracelet line detection and
recognition. Section V evaluates the performance of YOLOv5
used for detection and the machine learning algorithms used
for recognition. Conclusion drawn from our work is present in
Section VI followed by space for future work in Section VII.
II. RELATED WORK
Identification from images turns out to be extremely difficult
assuming there are no obvious attributes accessible like faces
or tattoos. Thumbprints or fingerprints are one of the oldest
and most widely used methods used for recognition. [7] paper
used thumbprints for recognition in which they are using an
8-bit grey level image. Each bit plane is then taken as input
by the neural network layer for recognition. [8], [9] and [10]
papers are based on contactless fingerprint detection using
different methods such as developing their model for detection,
reducing the deformations caused during image capturing and
making a proper device for fingerprint recognition respectively.
But nowadays many new ways for identification have been
discovered. A few ongoing examinations have looked for new
biometric characteristics, palm veins [11]. [12] paper presents
an acclimatization and adjustment test to assess the convenience of a contactless hand biometric framework according
to the user’s point of view. [13] used smartphones for detecting
wrist veins for phone unlocking, making online payments, and
bank account verification. The major drawback of using palm
or wrist veins for identification is the possibility of thick fatty
tissue obstructing the subcutaneous blood vessels. In such a
scenario capturing pictures will be difficult.
Some of the other techniques for biometric identification
include skin marks [14], androgenic hair [15], and hand victory
sign patterns [16], for handling the situation where accessing
faces or tattoos is difficult. Existing palmprint recognition
techniques are [17], [18], [19], [20], [21] for matching nonlatent palmprint pictures taken from digital cameras centre
around commercial biometric application with client collaboration and very much controlled imaging conditions.
In addition to palmprint, other comparable hand-based
biometrics proposed in the writing incorporate, e.g., finger
surface [22] and finger-knuckle-print [23] [17] in very much
controlled and helpful conditions. Vein acknowledgment is
chiefly utilized for business applications, where vein pictures
are caught under infrared light in very much controlled
conditions. Recently, specialists show that a few veins are
concealed in various pictures and can be visualized for forensic
investigation. The vein recognition methods depend on high
picture goal and clear visualized veins, which are in some
cases difficult to be extracted due to high concentration of fat
or melanin or low image. [24]
Skin marks are relevant to high-resolution images only and
androgenic hair doesn’t necessarily in every case happen in
some body parts e.g., wrists. Victory signs, palmprints and
finger knuckles are not generally seen in recognition. Then
again, the wrist might probably be noticed even though the
subjects are wearing long sleeves, welcoming or saluting.
Bracelet lines can be used for forensic identification [6]
[25]. Our paper focuses on using bracelet line detection and
recognition using machine learning for business purposes.
III. METHODOLOGY
In this section, the method for data collection, bracelet line
detection and recognition is described. In the first phase, data
collection is done through various sensors. The technologies
used for the data collection process are as follows:
A. Technology Used
1) Arduino Uno R3: Arduino Uno R3 is a small microcontroller, with an 8-bit processor which ATmega328p,
consisting of 14 Digital Pins that can be used as input
and output out of which 6 can be used as PWM outputs
and also, has 6 analog input pins. With all these ports it
can support multiple communication protocols such as
UART, SPI and I2C.
For power, it can take power from the USB-B connector
or the Barrel Plug and even from the Vin Port. What
makes it unique for prototyping, it has another power
processor which prevents damage on the board, and
even in worst cases only has a dead pin, unlike other
microcontrollers which often have burnouts on the entire
board, making this an excellent choice for prototyping.
2) ESP32-CAM: This is a small-sized, OV2640 camera
module that uses the ESP32 processor which is a 32-bit
processor and has a TF card slot onboard. It is widely
used for video monitoring, image uploads etc. having
an inbuilt functionality for Wi-Fi and Bluetooth along
with multiple communication protocols, it is extremely
handy for small applications and even has low power
consumption due to multiple sleep modes with current
as low as 6mA.
Due to its small size and features, it is extremely handy
for this project and helps for a seamless connection.
3) Communication Between Arduino Uno R3 and
ESP32-CAM: For the communication between the two
boards, Tx and Rx ports are used on both. The Arduino
is then used as a dummy microcontroller, which enables
us to program the ESP32-CAM which is, in turn, acting
as a dummy controller to be programmed as we connect
the GPIO 0 pin to the Ground. Once the programming
is done using the UART protocol, the GPIO 0 pin is
removed from the ground. The Arduino now acts as a
power source as well as a good interface for the testing
as we can use the Arduino IDE to generate the links and
to route the data from the ESP32-CAM to the Server
needed.
This entire setup is very convenient as it allows the same
microcontrollers to be re-used through the entire process,
as well as for the deployment.
4) Arduino IDE: The Arduino IDE is one of the best and
most widely used IDE for the programming of small
microcontrollers. It has multiple enabled functions for
plotting and monitoring.
It can be used for almost all the sensors as well as small
microcontrollers, using different libraries. This IDE is
easy to install and can be used on the go, using C/C++
for programming.
B. Setup
For the prototyping setup, the ESP32-CAM is connected
with the Arduino Uno R3, where the Arduino is a dummy
microcontroller along with ESP32-CAM which is, in turn, a
dummy microcontroller while the IDE loads the code, this is
achieved by grounding the RESET pin of the Arduino Uno
R3 and by grounding the GPIO 0 pin of the ESP32-CAM as
shown in figure 1 below.
This makes Arduino the perfect interface for programming
the ESP32-CAM. Once the code is uploaded, the GPIO 0 pin
is removed from the ground as shown in figure 2. Next, the
Arduino IDEs serial monitor can be used to get the generated
URL from the ESP32-CAM, which is used to see the image,
this can be adjusted to send to the server. Arduino also acts
as a constant power source for the ESP32-CAM.
Once, the prototyping is done and the device is ready to be
deployed the setup now consists of the Arduino Uno R3 as a
Figure 1: Circuit diagram with GPIO0 pin grounded
Figure 2: Circuit diagram with GPIO0 pin not grounded
power source for the ESP32-CAM as shown in figure 3.
Next, the ESP32-CAM sends the data to the server through
Wi-Fi connectivity where further processing and recognition
are done on the image.
C. Data Collection
Dataset Collection is one of the most crucial points in any
work, as it determines how well the models get trained, if the
dataset is not accurate it can cause failures. Thereby, all the
dataset images were taken with extreme caution to avoid any
corrupt or bad data.
For the dataset of over 1400+ images, images of both the
wrists of people were taken so that detection and recognition
techniques can be performed.
The dataset consisted of multiple age groups of people,
ranging from 15-year-old to 60-year-old people. This is to
get wider subject data for better analysis. Some of the sample
data is shown in the figure 5.
The images were taken at multiple angles and multiple
distances from the wrist [26], which enables better training of
Figure 3: Circuit diagram during deployment
Figure 4: Actual Setup
the detection algorithm which is further used in the recognition
algorithm.
D. Bracelet Lines detection using Yolov5
The collected data samples are fed to the Yolov5 model for
detecting bracelet lines and then to machine learning models
for training. You only look once (YOLO) is a state-of-the-art,
real-time object detection system. YOLO architecture is more
like FCNN (fully convolutional neural network) and passes the
image (n×n) once through the FCNN and the output is (m×
m) prediction. The architecture is splitting the input image in
(m×m) grid and for each grid generation two bounding boxes
and class probabilities for those bounding boxes. YOLO trains
on full images and directly optimises detection performance.
The YOLO design empowers start-to-finish preparation and
real-time speeds while keeping up with high accuracy. This
unified model has several benefits over traditional methods of
object detection, some of them are listed below:
1) YOLO is extremely fast. Since the detection is framed as
a regression problem, a complex pipeline is not required.
The neural network is run on a new image at test time to
predict detections. The base network runs at 45 frames
per second with no batch processing on a T itan×GP U
and a fast version runs at more than 150 fps. This means
streaming video can be processed in real-time with less
than 25 milliseconds of latency.
2) YOLO reasons globally about the image when making
predictions. Unlike sliding window and region proposalbased techniques, YOLO sees the entire image during
training and test time so it implicitly encodes contextual
information about classes and their appearance.
3) YOLO learns generalizable representations of objects.
When trained on natural images and tested on the
Figure 5: Sample Dataset
artwork, YOLO outperforms top detection methods like
Deformable Part Model (DPM) and Region-Based Convolutional Neural Network (R-CNN) by a wide margin.
Since YOLO is exceptionally generalizable it is less
likely to break down when applied to new domains or
unexpected inputs.
YOLO Version 5 launched in 2020 by Ultralytics [27] is
considered in this work and is currently the most advanced
object identification algorithm available. YOLOv5 is different
from all other prior releases, as this is a PyTorch execution as
opposed to a fork from the first Darknet. Same as YOLOv4,
the YOLOv5 has a CSP backbone and PA-NET neck. The
significant upgrades incorporate mosaic information expansion
and auto-learning bouncing box secures. It is about 88%
smaller than YOLOv4 (27 MB vs 244 MB), 180% faster than
YOLOv4 (140 FPS vs 50 FPS) and is roughly as accurate as
YOLOv4 on a similar errand (0.895 mAP vs 0.892 mAP).
YOLOv5 is customised in this work to only identify bracelet
lines and no other objects. We have provided the YOLOv5
model with our dataset of images of hands and their corresponding wrist labels for training and testing. We changed the
YAML file with our customised file giving several classes as
one and class name as ’wrist’. We trained the model with
850 images and 85 images for validation with 100 epochs.
YOLOv5 uses several hidden layers and assigns weights
accordingly. We have used the weight which gives the best
result among all the epochs. Next, we use these weights to
detect a new bracelet line. A few examples of detected bracelet
lines are shown in figure 6. Once the wrist is identified, it is
cropped and sent to machine learning models for recognition.
E. Classification
The data is split into the ratio of 0.3 for testing and training
for all the classifiers. The seven machine learning models
considered in the work are as follows:
1) Logistic Regression: Logistic regression is the appropriate regression analysis when the dependent variable is binary.
Like all regression analyses, logistic regression is a predictive
analysis. It is used to display information and to explain the
connection between one dependent variable and at least one
independent variable. Rather than fitting a straight line or
hyperplane, the logistic regression model purposes the logistic
function to press the result of a linear equation. Logistic
regression estimates the probability of an event based on a
given dataset of independent variables. As the outcome is
a probability, the dependent variable is bounded between 0
and 1. A logit transformation is applied to the odds, i.e., the
probability of success divided by the probability of failure.
This is also commonly known as the natural logarithm of odds,
or the log odds. The logistic function is defined as:
logistic(µ) = 1
1 + exp (µ)
(1)
, which is a sigmoid function and
logit(p) =
log p(y = l)
1 − (p = l)
= β0 + β1.x1 + β2.x2 + ... + βp.xp (2)
for l = 1. . . n.
In this logistic regression equation, logistic(µ) is the dependent or response variable and x is the independent variable.
The beta coefficient in this model is estimated by Maximum
Likelihood Estimation (MLE). This method tests various values of beta through repeated iterations to optimize for the
best fit of log odds. All of these iterations produce the loglikelihood function, and logistic regression tries to maximize
this function to find the best parameter estimate. When the
Figure 6: Detection of Bracelet Lines from Hand
ideal coefficients are found, the conditional probabilities for
each observation can be calculated, logged, and added together
to get a predicted probability.
2) Support Vector Machine: Support Vector Machines
(SVM) is a fast and dependable classification algorithm that
performs very well when there is a shortage of data to analyze.
SVM can categorize new text once it has given a set of
labelled training data for each category. They have two main
advantages as compared to newer algorithms: higher speed and
better performance with less number of samples. This makes
the algorithm very suitable for text classification problems,
where it’s common to have access to a dataset of at most a
couple of thousands of tagged samples.
The solution to the support vector classifier involves only
the inner products of the observations. The product of two
vectors a and b is defined as
(a, b) = Xr
i=1
aibi (3)
Thus the inner product of the two observations xi
, xi
′ is
given by
(xi
, xi
′ ) = Xp
j=1
xijxi
′j (4)
It can be shown that • The linear SVC can be represented
as
f(x) = β0 +
Xn
i=1
αi(xi
, xi
′ ) (5)
where there are n parameters αi
, i = 1,...,n, one per training
observation. • To estimate the parameters α1,...,αn and β0, all
we need are the n(n − 1)/2 inner products xi
, x
′
i
between all
pairs of training observations.
An SVM takes data points and outputs the hyperplane that
best separates the tags. This line is the decision boundary:
anything that falls to one side of it we will classify as one
tag, and anything that falls to the other as another tag. For
SVM, the hyperplane is the one that maximizes the margins
from both tags. In other words: the hyperplane (like line,
circle) whose distance to the nearest element of each tag is
the largest. When the data is not linearly separable, we add
a third dimension to separate the tags. The idea of mapping
higher dimensions to the model can get pretty computationally
expensive. There can be a lot of new dimensions and each one
of them might involve a complicated calculation. Doing this
for every vector in the dataset can be a lot of work, so it will
be better to find a cheaper solution.
3) Gaussian Na¨ıve Bayes Classifier: Na¨ıve Bayes is a
probability-based machine learning algorithm based on the
Bayes Theorem, used in a wide variety of classification tasks.
Bayes’ Theorem, named after Reverend Thomas Bayes, is a
mathematical formula used to calculate conditional probabilities. Bayes theorem determines the probability of event A
given that event B has already occurred.
P(A|B) = P(B|A).P(A)
P(B)
(6)
The above formula tells us how often A happens given
that B happens, written as P(A—B) also called posterior
probability. P(B—A) represents how often B happens given
that A happens. P(A) and P(B) represent the happening
of event A and event B on their own respectively. Bayes’
Theorem is simply a way to find a probability when certain
other probabilities are known. In Gaussian Na¨ıve Bayes, we
assume that continuous values associated with each feature
are distributed according to a Gaussian distribution. When a
Gaussian distribution curve is plotted, it forms a bell-shaped
curve that is symmetric to the mean of the feature values as
shown:
If any feature contains numerical values instead of categories then it can be transformed into categorical counterparts
before creating their frequency tables. The other option can
be to use the distribution of the numerical variable to have a
good guess of the frequency. Let’s assume a training data set
containing X continuous attributes. The data is divided by the
target value. The mean µ and variance σ
2 of X is calculated
for each target value according to the given formula.
µ =
1
n
Xn
i=1
xi (7)
σ
2 =
1
n − 1
Xn
i=1
(xi − µ)
2
(8)
Conditional probability from some observation value x
whose features are assumed to be Gaussian can be given by:
P(x) = 1
√
2πσ2
e
− (x − µ)
2
2σ
2
(9)
Na¨ıve Bayes algorithms are based on over-simplified assumptions yet have worked quite well in many real-world
situations like document classification, emotion analysis, spam
filtering, etc. They can be easily trained over a small dataset to
estimate the necessary parameters. Naive Bayes classifiers can
be much faster as compared to more sophisticated methods.
Their biggest disadvantage is the requirement for attributes to
be independent.
4) K- Nearest Neighbour: K-Nearest Neighbor is one of
the least difficult Machine Learning calculations based on
Supervised Learning techniques. K-NN algorithm expects the
likeness between the new information and available information and put the new case into the classification that is
generally like the accessible classifications.
K-NN stores every one of the accessible information and
groups another information point based on the similarity. This
implies when new information shows up then it tends to be
effectively characterized into a decent suite class by using KNN. K-NN is a non-parametric algorithm, and that implies it
makes no presumptions on essential information.
KNN algorithm at the training stage simply stores the
dataset and when it gets new information, then it classifies
that information into a classification that is much like the new
information.
There is no specific method for deciding the best incentive
for ”K”, so we want to try some values to find the best out of
them. The most favoured value for K is 5. An extremely low
value for K like K=1 or K=2, can be noisy and lead to the
impact of outliers in the model. Large values of K are good
but can make the model slow.
5) Decision Tree: A decision tree is a classifier where the
model mimics human decision-making logic. In a decision
tree, each internal node splits the training examples into at
least two sub-trees according to a specific condition. In the
most simple implementations, the split condition takes into
account a single predictor. The observations falling to each
leaf node ideally belong to a particular category.
Every node relates to a specific property, and the branches
compare to a range of values. This range of values divides
the dataset according to a specific property. New nodes are
classified by traversing the tree, beginning from the root and
going down to a leaf, which is entirely determined by the
result of the condition at every node.
In particular, we start at the root node of a tree and consider
the node that relates to the root condition. We then determine
which branch the observed value of the given attribute corresponds to. The next node in our way is the one toward the
finish of the chosen branch. We repeat this process and traverse
the tree until we arrive at a leaf.
Note that decision tree splits can be based on both categorical and numeric variables. On account of numeric qualities,
decision trees can be mathematically interpreted as a collection
of hyperplanes, each symmetrical to one of the axes. Normally,
a less complicated decision tree is favoured, as it is easier to
understand and implement. Tree complexity vitally affects its
accuracy and generalisation capacity.
Large decision trees suffer from overfitting on the information and thus display poor accuracy on the test set. A big decision tree can sum up well to new patterns if it was prompted
without overfitting the information. The tree complexity is
constrained by the stopping criteria used to construct the tree
and the pruning technique that is utilised. Common measures
for the tree complexity include the following measurements:
(a) the total number of nodes and leaves
(b) tree depth
(c) the number of attributes used
6) Random Forest: In Random Forests, the trees are trained
on bootstrapped samples but at the time of splitting, only a
subset of the total number of features is considered i.e. for
each split to be made, m ¡ M predictors are considered where
M is the total number of features. As an accepted practice,
most practitioners prefer m to be nearly equal to the square
root of M.
This practice is useful in the presence of some very strong
predictors. If such predictors are present then each tree will
use these predictors during splitting to minimise the chosen
criterion and end up looking like each other. By decorrelating,
the trees we are forcing each of them to be trained on a
different set of features as well as different sets of instances.
Averaging these uncorrelated trees also results in a further
decrease in the overall variance of the model.
Random forests introduce variance in the trees by randomly
selecting a subset of given features at each “split point” while
making the trees. By forcing only a subset of features to be
considered, each decision tree is effectively trained on a more
different dataset. Due to this decorrelation, the collective error
made by the model is lower and the average performance is
often better than the individual trees.
Random Forests use bootstrap samples to train a collection
of decision trees. Of the training sample, a certain portion
(about one-third) of the data is set aside for validation.
This sample is also called an out-of-bag sample. Feature
bagging is also performed to introduce added randomness to
the ensemble. The outputs of each tree are then collectively
used to obtain the final prediction - the outputs are averaged
in case of regression and majority voting is performed for
classification tasks where the most frequent prediction (the
mode) is picked as the result. The out-of-bag sample is used
for cross-validation and finalising the predictor.
7) Convolutional Neural Network: A convolutional neural
network, or CNN, is a deep learning neural network intended
for handling structured arrays of data such as images. They
are broadly utilized in computer vision and have turned into
the cutting edge for the majority of visual applications like
image classification, and have also found success in natural
language processing for text classification.
A CNN model converts input data to yield class probabilities through a progression of a few hidden units that
incorporate convolutional, pooling, and fully connected layers
or also known as dense layers. In our work we have used 4
convolutional neural network layers, followed by a flattened
layer and finally 5 dense layers. We have used a fixed input
size of the image, kernel size and activation function in all the
CNN layers. Several filters are changed from 16 to 32 to 64
and finally 128. We have flattened these 128 filter frames and
passed them to a 550 neuron dense layer. With each subsequent
dense layer, we have decreased these neurons in dense layers
from 550 to 200. Finally, we have combined this dense layer
into an output dense layer containing the number of neurons
same as the input classes using the activation function softmax.
We have also used dropout to check which filters work better
with our model.
F. Measures for comparison
The various performance measures considered for the comparison of the machine learning algorithms are as follows:
1) True Positive (TP)- Outcome when a model correctly
predicts the positive class sample as belonging to the
positive class.
2) True Negative (TN)- Outcome when a model correctly
predicts the negative class sample as belonging to the
negative class.
3) False Positive (FP)- Outcome when a model incorrectly
predicts the negative class sample as belonging to the
positive class.
4) False Negative (FN)- Outcome when a model incorrectly predicts the positive class sample as belonging to
the negative class.
Further, the four parameters used for the evaluation of the
machine learning algorithms are:
1) Accuracy - It is the ratio of the number of correct
predictions to the total number of input samples.
Accuracy =
T P + T N
T P + T N + F P + F N
2) Precision - Precision gives the proportion of identifications, actually correct.
P recision =
T P
T P + F P
3) Recall - Recall gives the proportion of actual positives,
identified correctly.
Recall =
T P
T P + F N
4) F1 Score - F1 Score is the harmonic mean of precision
and recall.
F1Score =
2 ∗ P recision ∗ Recall
P recision + Recall
Figure 7: Implementation Model of Our Work
IV. IMPLEMENTATION
In the proposed work, after the dataset was created, as
discussed in Section III-C, a YOLO algorithm-based model as
described in earlier Section III-D was created for the efficient
detection of bracelet lines on the wrist images.
This model provides very high accuracy of 99.5%, this
extraction helps for further recognition techniques as the
machine learning techniques rely heavily on the pre-processing
of the data. Once, the data was pre-processed, the weights file
is then sent for the training of the machine learning models.
In the working, the ESP32CAM captures the photograph
of the person’s wrist and then passes it on to the application,
where the bracelet lines are extracted from the image and sent
to the working machine learning model in our proposed work
it is a convolutional neural network and it was the best fit.
Many different machine learning models were trained using
the pre-processed data and were judged on different criteria
such as Accuracy, Precision as seen in Section III-E.
Once the image is sent, then the recognition model identifies
the person and therefore is noted and sent as the output.
Thereby, the recognition is done completely through a
contactless interface with a new technique. A basic implementation model is shown in the figure 7.
V. RESULTS AND DISCUSSION
In this section, results and a comparison of the various
machine learning algorithms for the detection and recognition
of bracelet lines are presented. The dataset consists of a total
of 1400+ right and left-hand images of 85 people. Let’s first
discuss the results of the detection model. A sample image
of training and validation data is shown in figure 8a. Our
customised YOLOv5 model was giving an accuracy of 99.5%.
The details result is present in figure 9.
Now talking about the recognition models. We have used
the above-detected images for image recognition. Table I
represents the result of our seven machine learning models.
As indicated by the results in the table, all seven algorithms
show a decent performance and give a comparable value for
all the measures. CNN performs slightly better than the other
(a) Training Batch (b) Validation Batch (c) Prediction
Figure 8: Training and Validation Sample
(a) Confusion Matrix (b) Labels (c) Labels Correlation (d) Result
(e) F1-score curve (f) Precision curve (g) Recall curve (h) Precision-Recall curve
Figure 9: Result of YOLOv5 bracelet line detection model
Table I: Result of Machine Learning Models
Accuracy Precision Recall F1 Score
Logistic Regression 0.804 0.783 0.799 0.768
SVM 0.824 0.821 0.823 0.799
Gaussian NBC 0.781 0.791 0.776 0.766
KNN 0.801 0.806 0.804 0.778
Decision Tree 0.585 0.582 0.579 0.547
Random Forest 0.842 0.807 0.831 0.802
CNN 0.988 0.978 0.982 0.988
algorithms. The accuracy vs epoch curve of CNN is given in
the figure 10. Random forest gives less accuracy than CNN but
by increasing the depth of the tree we can increase its accuracy.
KNN, SVM and Logistic Regression give a close value for all
the four parameters. Gaussian Naive Bayes classifier gives a
slightly lesser accuracy. The decision tree performs the lowest
Figure 10: Accuracy versus Epoch curve
of all the algorithms.
Figure 11: Loss versus Epoch curve
VI. CONCLUSION
The study of human bracelet line detection and recognition
has many applications. Different body parts can be used for
personal identification like veins, fingerprints, palmprints etc.
In this work, bracelet lines were used for the data collection.
We took the images of wrists of over 85 people. We used
the YOLOv5 detection algorithm as it is the most recent and
gives phenomenal results. As much work is not done in the
field of recognition using bracelet lines so we used several
machine learning algorithms to see which gives the better
result. Convolutional Neural Networks give the highest result
in all 4 parameters, i.e., accuracy, precision, recall and f1-
score. We can say that we have successfully achieved all our
aims as mentioned in Section I.
VII. FURTHER WORKS
Using the current proposed work, it is now easy to identify
a person using the wrist. For the next step, it is intended that
we may be able to identify a person of interest through footage
cams and keep track of the person of interest.
As this technique can enable us to identify a person with
only some dataset images, it is possible to extract the wrist
images from the cameras and then used them for this purpose.
It can help keep track at any camera point and thereby, can
be used for various purposes such as security, protection of
assets, etc.
REFERENCES
[1] Alberto de Santos-Sierra, Carmen Sanchez-Avila, Gonzalo Bailador ´
Del Pozo, and Javier Guerra-Casanova. Unconstrained and contactless
hand geometry biometrics. Sensors, 11(11):10143–10164, 2011.
[2] Kevin HM Cheng and Ajay Kumar. Contactless biometric identification
using 3d finger knuckle patterns. IEEE transactions on pattern analysis
and machine intelligence, 42(8):1868–1883, 2019.
[3] Pierre Baldi and Yves Chauvin. Neural networks for fingerprint
recognition. neural computation, 5(3):402–418, 1993.
[4] Yi Sun, Ding Liang, Xiaogang Wang, and Xiaoou Tang. Deepid3:
Face recognition with very deep neural networks. arXiv preprint
arXiv:1502.00873, 2015.
[5] Jaime Uriarte-Antonio, Daniel Hartung, J Enrique Suarez Pascual, and
Raul Sanchez-Reillo. Vascular biometrics based on a minutiae extraction
approach. In 2011 Carnahan Conference on Security Technology, pages
1–7. IEEE, 2011.
[6] Wojciech Michal Matkowski, Frodo Kin Sun Chan, and Adams Wai Kin
Kong. A study on wrist identification for forensic investigation. Image
and Vision Computing, 88:96–112, 2019.
[7] MR Ramlan, DBL Bong, and TZ Lee. Analysis of thumbprint recognition in different bit levels. In 2012 International Conference on
Advanced Computer Science Applications and Technologies (ACSAT),
pages 192–196. IEEE, 2012.
[8] Chenhao Lin and Ajay Kumar. Matching contactless and contact-based
conventional fingerprint images for biometrics identification. IEEE
Transactions on Image Processing, 27(4):2008–2021, 2018.
[9] Ruggero Donida Labati, Angelo Genovese, Vincenzo Piuri, and Fabio
Scotti. Contactless fingerprint recognition: A neural approach for
perspective and rotation effects reduction. In 2013 IEEE Symposium
on Computational Intelligence in Biometrics and Identity Management
(CIBIM), pages 22–30, 2013.
[10] Ruggero Donida Labati, Vincenzo Piuri, and Fabio Scotti. Neural-based
quality measurement of fingerprint images in contactless biometric systems. In The 2010 International Joint Conference on Neural Networks
(IJCNN), pages 1–8, 2010.
[11] Hengyi Zhang, Chaoying Tang, Adams Wai-Kin Kong, and Noah Craft.
Matching vein patterns from color images for forensic investigation.
In 2012 IEEE Fifth International Conference on Biometrics: Theory,
Applications and Systems (BTAS), pages 77–84. IEEE, 2012.
[12] Goh Kah Ong Michael, Tee Connie, and Andrew Teoh Beng Jin. A
preliminary acclimatization study of a contactless biometrics using palm
vein feature. In 2011 6th IEEE Conference on Industrial Electronics and
Applications, pages 1022–1027. IEEE, 2011.
[13] Raul Garcia-Martin and Raul Sanchez-Reillo. Vein biometric recognition
on a smartphone. IEEE Access, 8:104801–104813, 2020.
[14] Arfika Nurhudatiana and Adams Wai-Kin Kong. On criminal identification in color skin images using skin marks (rppvsm) and fusion with
inferred vein patterns. IEEE Transactions on Information Forensics and
Security, 10(5):916–931, 2015.
[15] Frodo Kin Sun Chan and Adams Wai-Kin Kong. Using hair follicles
with leg geometry to align androgenic hair patterns. In 2015 European
Intelligence and Security Informatics Conference, pages 137–140. IEEE,
2015.
[16] Ahmad Hassanat, Mahmoud B Alhasanat, Mohammad Ali Abbadi,
Eman Btoush, Mouhammd Al-Awadi, and Ahmad S Tarawneh. Victory sign biometric for terrorists identification. arXiv preprint
arXiv:1602.08325, 2016.
[17] Michał Choras and Rafał Kozik. Contactless palmprint and knuckle ´
biometrics for mobile devices. Pattern Analysis and Applications,
15(1):73–85, 2012.
[18] Zhenan Sun, Tieniu Tan, Yunhong Wang, and Stan Z Li. Ordinal
palmprint represention for personal identification [represention read
representation]. In 2005 IEEE computer society conference on computer
vision and pattern recognition (CVPR’05), volume 1, pages 279–284.
IEEE, 2005.
[19] Xiangqian Wu, Qiushi Zhao, and Wei Bu. A sift-based contactless
palmprint verification approach using iterative ransac and local palmprint
descriptors. Pattern recognition, 47(10):3314–3326, 2014.
[20] Wenxiong Kang and Qiuxia Wu. Contactless palm vein recognition
using a mutual foreground-based local binary pattern. IEEE transactions
on Information Forensics and Security, 9(11):1974–1985, 2014.
[21] Lunke Fei, Guangming Lu, Wei Jia, Shaohua Teng, and David Zhang.
Feature extraction methods for palmprint recognition: A survey and
evaluation. IEEE Transactions on Systems, Man, and Cybernetics:
Systems, 49(2):346–363, 2018.
[22] Damon L Woodard and Patrick J Flynn. Personal identification utilizing
finger surface features. In 2005 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR’05), volume 2, pages
1030–1036. IEEE, 2005.
[23] Lin Zhang, Lei Zhang, David Zhang, and Hailong Zhu. Online
finger-knuckle-print verification for personal authentication. Pattern
recognition, 43(7):2560–2571, 2010.
[24] Raul Garcia-Martin and Raul Sanchez-Reillo. Wrist vascular biometric
recognition using a portable contactless system. Sensors, 20(5):1469,
2020.
[25] Kennedy Chinedu Okafor and Omowunmi Mary Longe. Computational
wrist-print biometric identification system using discrete cosine transform. In International Conference on Computational Science and Its
Applications, pages 460–475. Springer, 2021.
[26] J Enrique Suarez Pascual, Jaime Uriarte-Antonio, Raul Sanchez-Reillo,
and Michael G Lorenz. Capturing hand or wrist vein images for biometric authentication using low-cost devices. In 2010 Sixth International
Conference on Intelligent Information Hiding and Multimedia Signal
Processing, pages 318–322. IEEE, 2010.
[27] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You
only look once: Unified, real-time object detection. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pages
779–788, 2016.
